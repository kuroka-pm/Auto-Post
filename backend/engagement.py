"""
engagement.py â€” æŠ•ç¨¿å±¥æ­´ã®è¨˜éŒ² & Xã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ CSV ã‚¤ãƒ³ãƒãƒ¼ãƒˆ & AIåˆ†æ

Phase 3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ï¼ˆç„¡æ–™ç‰ˆï¼‰
- æŠ•ç¨¿çµæœã‚’ post_history.json ã«ä¿å­˜
- Xã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã® CSV ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆ$0ï¼‰
- Gemini ã§ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‚¾å‘ã‚’åˆ†æã—ã€æ¬¡å›ç”Ÿæˆã«æ´»ç”¨
"""

from __future__ import annotations

import csv
import json
import sys
import time
from pathlib import Path
from typing import Any

# ---------------------------------------------------------------------------
# æŠ•ç¨¿å±¥æ­´ã®ä¿å­˜å…ˆï¼ˆexe å¯¾å¿œ: config_manager.py ã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
# ---------------------------------------------------------------------------

if getattr(sys, 'frozen', False):
    _BASE_DIR = Path(sys.executable).resolve().parent
else:
    _BASE_DIR = Path(__file__).resolve().parent

_HISTORY_FILE = _BASE_DIR / "post_history.json"
_DAILY_OVERVIEW_FILE = _BASE_DIR / "daily_overview.json"
_ANALYSIS_CACHE_FILE = _BASE_DIR / "analysis_cache.json"
_INBOX_DIR = _BASE_DIR / "INBOX"

# INBOX ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°è‡ªå‹•ç”Ÿæˆ
_INBOX_DIR.mkdir(exist_ok=True)


def _load_history() -> list[dict]:
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if _HISTORY_FILE.exists():
        try:
            return json.loads(_HISTORY_FILE.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, OSError):
            return []
    return []


def get_post_history() -> list[dict]:
    """å…¬é–‹APIç”¨: æŠ•ç¨¿å±¥æ­´ã‚’è¿”ã™ã€‚"""
    return _load_history()


def _save_history(history: list[dict]) -> None:
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚"""
    _HISTORY_FILE.write_text(
        json.dumps(history, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


def get_post_history() -> list[dict]:
    """æŠ•ç¨¿å±¥æ­´ã‚’å–å¾—ã™ã‚‹ï¼ˆUIè¡¨ç¤ºç”¨ã€æ–°ã—ã„é †ï¼‰ã€‚"""
    return list(reversed(_load_history()))


# ---------------------------------------------------------------------------
# æŠ•ç¨¿è¨˜éŒ²ï¼ˆã‚¢ãƒ—ãƒªã‹ã‚‰æŠ•ç¨¿ã—ãŸåˆ†ï¼‰
# ---------------------------------------------------------------------------

def record_post(
    post_text: str,
    post_id: str | None = None,
    platform: str = "x",
    style_name: str = "",
    trend_used: str = "",
    smart_analysis: bool = False,
) -> dict:
    """æŠ•ç¨¿ã‚’å±¥æ­´ã«è¨˜éŒ²ã™ã‚‹ã€‚"""
    entry = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "epoch": int(time.time()),
        "platform": platform,
        "post_id": post_id,
        "text": post_text,
        "char_count": len(post_text),
        "style": style_name,
        "trend": trend_used,
        "smart_analysis": smart_analysis,
        "source": "app",
        "engagement": None,
    }

    history = _load_history()
    history.append(entry)

    # æœ€å¤§500ä»¶ã«åˆ¶é™
    if len(history) > 500:
        history = history[-500:]

    _save_history(history)
    return entry


# ---------------------------------------------------------------------------
# é‡è¤‡é˜²æ­¢ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ---------------------------------------------------------------------------

def get_recent_note_urls(days: int = 3) -> set[str]:
    """ç›´è¿‘ N æ—¥é–“ã«æŠ•ç¨¿ã—ãŸ note è¨˜äº‹ã® URL ã‚’è¿”ã™ã€‚"""
    cutoff = int(time.time()) - days * 86400
    urls: set[str] = set()
    for entry in _load_history():
        if entry.get("epoch", 0) >= cutoff:
            text = entry.get("text", "")
            # note.com ã® URL ã‚’æŠ½å‡º
            for word in text.split():
                if "note.com/" in word:
                    urls.add(word.strip("()[]ã€Œã€"))
    return urls


def get_recent_styles(count: int = 10) -> list[str]:
    """ç›´è¿‘ N ä»¶ã®æŠ•ç¨¿ã§ä½¿ç”¨ã•ã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«åã‚’è¿”ã™ã€‚"""
    history = _load_history()
    return [
        e.get("style", "") for e in history[-count:]
        if e.get("source") == "app" and e.get("style")
    ]

# ---------------------------------------------------------------------------
# Daily Overview ãƒ‡ãƒ¼ã‚¿ç®¡ç†
# ---------------------------------------------------------------------------

def _load_daily_overview() -> list[dict]:
    """æ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if _DAILY_OVERVIEW_FILE.exists():
        try:
            return json.loads(_DAILY_OVERVIEW_FILE.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, OSError):
            return []
    return []


def _save_daily_overview(data: list[dict]) -> None:
    """æ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€ã€‚"""
    _DAILY_OVERVIEW_FILE.write_text(
        json.dumps(data, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


def get_daily_overview() -> list[dict]:
    """æ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ï¼ˆUIè¡¨ç¤ºç”¨ã€æ–°ã—ã„é †ï¼‰ã€‚"""
    return list(reversed(_load_daily_overview()))


# ---------------------------------------------------------------------------
# CSV ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆXã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‹ã‚‰ï¼‰
# ---------------------------------------------------------------------------

def _parse_int(val: str) -> int:
    """CSV å€¤ã‚’å®‰å…¨ã« int ã«å¤‰æ›ã™ã‚‹ã€‚"""
    try:
        return int(val.strip().replace(",", ""))
    except (ValueError, AttributeError):
        return 0


def detect_csv_type(csv_path: str | Path) -> str:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹ã€‚

    Returns:
        "content"  = æŠ•ç¨¿å˜ä½ CSV (account_analytics_content_*.csv)
        "overview" = æ—¥æ¬¡æ¦‚è¦ CSV (account_overview_analytics.csv)
        "unknown"  = åˆ¤å®šä¸èƒ½
    """
    csv_path = Path(csv_path)
    for enc in ("utf-8-sig", "utf-8", "cp932", "shift_jis"):
        try:
            with open(csv_path, encoding=enc, newline="") as f:
                reader = csv.DictReader(f)
                headers = reader.fieldnames or []
            break
        except (UnicodeDecodeError, UnicodeError):
            continue
    else:
        return "unknown"

    if "ãƒã‚¹ãƒˆID" in headers:
        return "content"
    elif "Date" in headers and "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°" in headers:
        return "overview"
    return "unknown"


def import_csv(csv_path: str | Path | None = None) -> dict[str, int]:
    """Xã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹CSVã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å±¥æ­´ã«ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚

    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚None ã®å ´åˆã¯ INBOX å†…ã®æœ€æ–°CSVã‚’ä½¿ç”¨ã€‚

    Returns:
        {"imported": æ–°è¦ä»¶æ•°, "updated": æ›´æ–°ä»¶æ•°, "skipped": ã‚¹ã‚­ãƒƒãƒ—ä»¶æ•°}
    """
    if csv_path is None:
        csv_path = _find_latest_csv()
        if csv_path is None:
            raise FileNotFoundError(
                f"INBOX ãƒ•ã‚©ãƒ«ãƒ€ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {_INBOX_DIR}"
            )
    csv_path = Path(csv_path)

    # CSV èª­ã¿è¾¼ã¿ï¼ˆBOM å¯¾å¿œï¼‰
    rows = []
    for enc in ("utf-8-sig", "utf-8", "cp932", "shift_jis"):
        try:
            with open(csv_path, encoding=enc, newline="") as f:
                reader = csv.DictReader(f)
                rows = list(reader)
            break
        except (UnicodeDecodeError, UnicodeError):
            continue

    if not rows:
        raise ValueError(f"CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {csv_path}")

    history = _load_history()
    existing_ids = {h.get("post_id") for h in history if h.get("post_id")}

    imported = 0
    updated = 0
    skipped = 0

    for row in rows:
        post_id = row.get("ãƒã‚¹ãƒˆID", "").strip()
        if not post_id or post_id == "":
            skipped += 1
            continue

        engagement = {
            "impressions": _parse_int(row.get("ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°", "0")),
            "likes": _parse_int(row.get("ã„ã„ã­", "0")),
            "engagement": _parse_int(row.get("ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ", "0")),
            "bookmarks": _parse_int(row.get("ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", "0")),
            "shares": _parse_int(row.get("å…±æœ‰ã•ã‚ŒãŸå›æ•°", "0")),
            "follows": _parse_int(row.get("æ–°ã—ã„ãƒ•ã‚©ãƒ­ãƒ¼", "0")),
            "replies": _parse_int(row.get("è¿”ä¿¡", "0")),
            "retweets": _parse_int(row.get("ãƒªãƒã‚¹ãƒˆ", "0")),
            "profile_clicks": _parse_int(row.get("ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ•°", "0")),
            "detail_clicks": _parse_int(row.get("è©³ç´°ã®ã‚¯ãƒªãƒƒã‚¯æ•°", "0")),
            "url_clicks": _parse_int(row.get("URLã®ã‚¯ãƒªãƒƒã‚¯æ•°", "0")),
        }

        # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®æ›´æ–°
        if post_id in existing_ids:
            for h in history:
                if h.get("post_id") == post_id:
                    h["engagement"] = engagement
                    h["engagement_updated"] = int(time.time())
                    updated += 1
                    break
        else:
            # æ–°è¦ã‚¨ãƒ³ãƒˆãƒª
            text = row.get("ãƒã‚¹ãƒˆæœ¬æ–‡", "").strip()
            date_str = row.get("æ—¥ä»˜", "").strip()

            entry = {
                "timestamp": date_str,
                "epoch": 0,
                "platform": "x",
                "post_id": post_id,
                "text": text,
                "char_count": len(text),
                "style": "",
                "trend": "",
                "smart_analysis": False,
                "source": "csv_import",
                "engagement": engagement,
                "engagement_updated": int(time.time()),
            }
            history.append(entry)
            imported += 1

    # æœ€å¤§500ä»¶ã«åˆ¶é™
    if len(history) > 500:
        history = history[-500:]

    _save_history(history)
    return {"imported": imported, "updated": updated, "skipped": skipped}


def import_daily_overview_csv(csv_path: str | Path) -> dict[str, int]:
    """æ—¥æ¬¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¦‚è¦CSVã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã€‚

    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        {"imported": æ–°è¦ä»¶æ•°, "updated": æ›´æ–°ä»¶æ•°, "skipped": ã‚¹ã‚­ãƒƒãƒ—ä»¶æ•°}
    """
    csv_path = Path(csv_path)

    # CSV èª­ã¿è¾¼ã¿ï¼ˆBOM å¯¾å¿œï¼‰
    rows = []
    for enc in ("utf-8-sig", "utf-8", "cp932", "shift_jis"):
        try:
            with open(csv_path, encoding=enc, newline="") as f:
                reader = csv.DictReader(f)
                rows = list(reader)
            break
        except (UnicodeDecodeError, UnicodeError):
            continue

    if not rows:
        raise ValueError(f"CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {csv_path}")

    existing = _load_daily_overview()
    existing_dates = {d.get("date") for d in existing}

    imported = 0
    updated = 0
    skipped = 0

    for row in rows:
        date_str = row.get("Date", "").strip()
        if not date_str:
            skipped += 1
            continue

        entry = {
            "date": date_str,
            "impressions": _parse_int(row.get("ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°", "0")),
            "likes": _parse_int(row.get("ã„ã„ã­", "0")),
            "engagement": _parse_int(row.get("ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ", "0")),
            "bookmarks": _parse_int(row.get("ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", "0")),
            "shares": _parse_int(row.get("å…±æœ‰ã•ã‚ŒãŸå›æ•°\\", "0")),
            "new_follows": _parse_int(row.get("æ–°ã—ã„ãƒ•ã‚©ãƒ­ãƒ¼", "0")),
            "unfollows": _parse_int(row.get("ãƒ•ã‚©ãƒ­ãƒ¼è§£é™¤", "0")),
            "replies": _parse_int(row.get("è¿”ä¿¡", "0")),
            "retweets": _parse_int(row.get("ãƒªãƒã‚¹ãƒˆ", "0")),
            "profile_visits": _parse_int(row.get("ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ•°", "0")),
            "posts_created": _parse_int(row.get("ãƒã‚¹ãƒˆã‚’ä½œæˆ", "0")),
            "video_views": _parse_int(row.get("å‹•ç”»å†ç”Ÿæ•°", "0")),
            "media_views": _parse_int(row.get("ãƒ¡ãƒ‡ã‚£ã‚¢ã®å†ç”Ÿæ•°", "0")),
            "imported_at": int(time.time()),
        }

        if date_str in existing_dates:
            for d in existing:
                if d.get("date") == date_str:
                    d.update(entry)
                    updated += 1
                    break
        else:
            existing.append(entry)
            imported += 1

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    existing.sort(key=lambda x: x.get("date", ""), reverse=True)

    # æœ€å¤§365æ—¥åˆ†ã«åˆ¶é™
    if len(existing) > 365:
        existing = existing[:365]

    _save_daily_overview(existing)
    return {"imported": imported, "updated": updated, "skipped": skipped}


def import_csv_auto(csv_path: str | Path) -> dict[str, Any]:
    """CSVã‚’è‡ªå‹•åˆ¤å®šã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã€‚

    Returns:
        {"type": "content"|"overview", "imported": N, "updated": N, "skipped": N}
    """
    csv_type = detect_csv_type(csv_path)
    if csv_type == "content":
        result = import_csv(csv_path)
        return {"type": "content", **result}
    elif csv_type == "overview":
        result = import_daily_overview_csv(csv_path)
        return {"type": "overview", **result}
    else:
        raise ValueError(
            "CSVã®å½¢å¼ã‚’åˆ¤å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "Xã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸCSVã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )


def _find_latest_csv() -> Path | None:
    """INBOX ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æœ€æ–°CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ã€‚"""
    if not _INBOX_DIR.exists():
        return None

    csvs = sorted(
        _INBOX_DIR.glob("*.csv"),
        key=lambda p: p.stat().st_mtime,
        reverse=True,
    )
    # account_overview ã¯é™¤å¤–ï¼ˆcontent ã®æ–¹ã‚’ä½¿ã†ï¼‰
    for c in csvs:
        if "overview" not in c.name.lower():
            return c
    return csvs[0] if csvs else None


# ---------------------------------------------------------------------------
# AI å‚¾å‘åˆ†æ
# ---------------------------------------------------------------------------

_ANALYSIS_PROMPT = """\
## ã‚¿ã‚¹ã‚¯
ä»¥ä¸‹ã¯ X (Twitter) ã®éå»ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆã„ã„ã­ãƒ»RTãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼‰ã®å‚¾å‘ã‚’åˆ†æã—ã€
æ¬¡å›ã®æŠ•ç¨¿ç”Ÿæˆã«æ´»ã‹ã›ã‚‹å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’5ã¤å‡ºã›ã€‚

## æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå€‹åˆ¥ãƒã‚¹ãƒˆï¼‰
{post_data}

{daily_section}

## åˆ†æãƒ«ãƒ¼ãƒ«
- ã€Œä¼¸ã³ãŸæŠ•ç¨¿ã€ã¨ã€Œä¼¸ã³ãªã‹ã£ãŸæŠ•ç¨¿ã€ã®é•ã„ã‚’æ˜ç¢ºã«æŒ‡æ‘˜ã™ã‚‹
- æ–‡ä½“ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã®ç¨®é¡ãƒ»æŠ•ç¨¿æ™‚é–“å¸¯ãƒ»æ–‡å­—æ•°ã®å‚¾å‘ã‚’è¦‹ã‚‹
- æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€æ›œæ—¥åˆ¥ã®ã‚¤ãƒ³ãƒ—ãƒ¬å‚¾å‘ã‚„ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›ã‚‚åˆ†æã™ã‚‹
- æŠ½è±¡çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ç¦æ­¢ã€‚å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’å‡ºã™
- æ•°å­—ã‚’æ ¹æ‹ ã«èªã‚Œ

## å‡ºåŠ›å½¢å¼
1è¡Œ1ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã€‚ç•ªå·ä»˜ãã€‚å„80å­—ä»¥å†…ã€‚æ—¥æœ¬èªã§ã€‚
"""


def analyze_engagement_trends(
    api_key: str,
    model: str = "gemini-2.5-flash",
) -> str:
    """éå»ã®æŠ•ç¨¿ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‚¾å‘ã‚’Geminiã§åˆ†æã™ã‚‹ã€‚"""
    from google import genai
    from google.genai import errors as genai_errors
    from google.genai.types import GenerateContentConfig

    history = _load_history()

    with_engagement = [
        h for h in history
        if h.get("engagement") is not None
    ]

    if len(with_engagement) < 3:
        return ("ğŸ“Š åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\n"
                f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå–å¾—æ¸ˆã¿: {len(with_engagement)}ä»¶ / æœ€ä½3ä»¶å¿…è¦\n"
                "Xã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‹ã‚‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€INBOXãƒ•ã‚©ãƒ«ãƒ€ã«å…¥ã‚Œã¦ãã ã•ã„ã€‚")

    # ç›´è¿‘30ä»¶ã‚’æ•´å½¢
    post_summaries = []
    for h in with_engagement[-30:]:
        eng = h["engagement"]
        summary = (
            f"[{h.get('timestamp', 'ä¸æ˜')}] "
            f"æ–‡å­—æ•°:{h.get('char_count', 0)} "
            f"IMP:{eng.get('impressions', 0)} "
            f"â™¥:{eng.get('likes', 0)} "
            f"RT:{eng.get('retweets', 0)} "
            f"ã‚¨ãƒ³ã‚²:{eng.get('engagement', 0)} "
            f"æœ¬æ–‡: {h.get('text', '')[:60]}"
        )
        post_summaries.append(summary)

    post_data = "\n".join(post_summaries)

    # æ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
    daily_data = _load_daily_overview()
    daily_section = ""
    if daily_data:
        daily_lines = []
        for d in daily_data[:14]:  # ç›´è¿‘14æ—¥åˆ†
            net_follow = d.get("new_follows", 0) - d.get("unfollows", 0)
            daily_lines.append(
                f"[{d.get('date', '?')}] "
                f"IMP:{d.get('impressions', 0)} "
                f"â™¥:{d.get('likes', 0)} "
                f"ã‚¨ãƒ³ã‚²:{d.get('engagement', 0)} "
                f"æŠ•ç¨¿æ•°:{d.get('posts_created', 0)} "
                f"æ–°ãƒ•ã‚©ãƒ­ãƒ¼:{d.get('new_follows', 0)} "
                f"è§£é™¤:{d.get('unfollows', 0)} "
                f"ç´”å¢—:{net_follow:+d}"
            )
        daily_section = (
            "## æ—¥æ¬¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¦‚è¦ãƒ‡ãƒ¼ã‚¿\n"
            + "\n".join(daily_lines)
        )

    prompt = _ANALYSIS_PROMPT.format(
        post_data=post_data,
        daily_section=daily_section,
    )

    client = genai.Client(api_key=api_key)
    try:
        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=GenerateContentConfig(temperature=0.3),
        )
    except genai_errors.ClientError as e:
        msg = str(e)
        if "404" in msg or "not found" in msg.lower():
            response = client.models.generate_content(
                model="gemini-2.5-pro",
                contents=prompt,
                config=GenerateContentConfig(temperature=0.3),
            )
        else:
            raise

    return response.text.strip()


def get_history_summary() -> dict[str, Any]:
    """æŠ•ç¨¿å±¥æ­´ã®ã‚µãƒãƒªãƒ¼ã‚’è¿”ã™ï¼ˆæ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼å«ã‚€ï¼‰ã€‚"""
    history = _load_history()

    total = len(history)
    with_eng = [h for h in history if h.get("engagement") is not None]
    count = len(with_eng)

    total_likes = 0
    total_rt = 0
    total_imp = 0
    for h in with_eng:
        eng = h["engagement"]
        total_likes += eng.get("likes", 0)
        total_rt += eng.get("retweets", 0)
        total_imp += eng.get("impressions", 0)

    # ãƒ™ã‚¹ãƒˆã„ã„ã­
    best_likes = max(
        (h.get("engagement", {}).get("likes", 0) for h in with_eng),
        default=0,
    )

    result: dict[str, Any] = {
        "total_posts": total,
        "with_engagement": count,
        "avg_likes": round(total_likes / count, 1) if count else 0,
        "avg_retweets": round(total_rt / count, 1) if count else 0,
        "avg_impressions": round(total_imp / count, 1) if count else 0,
        "best_likes": best_likes,
    }

    # æ—¥æ¬¡ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ã®ã‚µãƒãƒªãƒ¼ã‚‚è¿½åŠ 
    daily = _load_daily_overview()
    if daily:
        total_daily_imp = sum(d.get("impressions", 0) for d in daily)
        total_new_follows = sum(d.get("new_follows", 0) for d in daily)
        total_unfollows = sum(d.get("unfollows", 0) for d in daily)
        days = len(daily)
        result["daily_overview"] = {
            "days": days,
            "total_impressions": total_daily_imp,
            "avg_daily_impressions": round(total_daily_imp / days) if days else 0,
            "total_new_follows": total_new_follows,
            "total_unfollows": total_unfollows,
            "net_follow_change": total_new_follows - total_unfollows,
            "best_day": max(daily, key=lambda d: d.get("impressions", 0)).get("date", "") if daily else "",
            "best_day_impressions": max(d.get("impressions", 0) for d in daily) if daily else 0,
        }

    return result


def get_feedback_for_prompt() -> str:
    """éå»ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æŠ•ç¨¿ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«
    çµ„ã¿è¾¼ã‚€ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡ã‚’ç”Ÿæˆã™ã‚‹ã€‚

    ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å½±éŸ¿ãªã—ï¼‰ã€‚
    """
    history = _load_history()
    with_eng = [
        h for h in history
        if h.get("engagement") is not None
        and h.get("engagement", {}).get("impressions", 0) > 0
    ]

    if len(with_eng) < 5:
        return ""

    # ã„ã„ã­æ•°ã§ã‚½ãƒ¼ãƒˆ â†’ ä¸Šä½ãƒ»ä¸‹ä½ã‚’æŠ½å‡º
    sorted_by_likes = sorted(
        with_eng,
        key=lambda h: h.get("engagement", {}).get("likes", 0),
        reverse=True,
    )

    top_posts = sorted_by_likes[:3]
    low_posts = [p for p in sorted_by_likes[-3:] if p.get("engagement", {}).get("likes", 0) == 0]

    # ä¼¸ã³ãŸæŠ•ç¨¿ã®ç‰¹å¾´
    top_lines = []
    for p in top_posts:
        eng = p["engagement"]
        top_lines.append(
            f"  - â™¥{eng.get('likes',0)} IMP{eng.get('impressions',0)} "
            f"({p.get('char_count',0)}å­—): {p.get('text','')[:50]}"
        )

    # ä¼¸ã³ãªã‹ã£ãŸæŠ•ç¨¿ã®ç‰¹å¾´
    low_lines = []
    for p in low_posts[:2]:
        eng = p["engagement"]
        low_lines.append(
            f"  - â™¥{eng.get('likes',0)} IMP{eng.get('impressions',0)} "
            f"({p.get('char_count',0)}å­—): {p.get('text','')[:50]}"
        )

    # æ–‡å­—æ•°ã®å‚¾å‘
    liked_chars = [p.get("char_count", 0) for p in top_posts if p.get("char_count")]
    avg_liked_chars = sum(liked_chars) // len(liked_chars) if liked_chars else 0

    parts = ["## éå»æŠ•ç¨¿ã®åˆ†æï¼ˆå‚è€ƒã«ã›ã‚ˆï¼‰"]
    parts.append("### ä¼¸ã³ãŸæŠ•ç¨¿ã®å‚¾å‘:")
    parts.extend(top_lines)
    if low_lines:
        parts.append("### ä¼¸ã³ãªã‹ã£ãŸæŠ•ç¨¿ã®å‚¾å‘:")
        parts.extend(low_lines)
    if avg_liked_chars:
        parts.append(f"### æ•°å€¤å‚¾å‘: é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸æŠ•ç¨¿ã®å¹³å‡æ–‡å­—æ•°ã¯{avg_liked_chars}å­—")

    return "\n".join(parts)


# ---------------------------------------------------------------------------
# åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥
# ---------------------------------------------------------------------------

def save_analysis_cache(analysis_text: str) -> None:
    """AIåˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚"""
    data = {
        "analysis": analysis_text,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    _ANALYSIS_CACHE_FILE.write_text(
        json.dumps(data, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


def load_analysis_cache() -> dict:
    """ä¿å­˜ã•ã‚ŒãŸAIåˆ†æçµæœã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    if _ANALYSIS_CACHE_FILE.exists():
        try:
            return json.loads(_ANALYSIS_CACHE_FILE.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, OSError):
            return {}
    return {}


# ---------------------------------------------------------------------------
# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨çµ±è¨ˆ
# ---------------------------------------------------------------------------

def get_dashboard_stats() -> dict:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨: ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã¨æœ€è¿‘ã®æŠ•ç¨¿ã‚’è¿”ã™ã€‚"""
    history = _load_history()
    today = time.strftime("%Y-%m-%d")

    # source=app ã®æŠ•ç¨¿ã ã‘ã‚’å¯¾è±¡ã«ã™ã‚‹
    app_posts = [h for h in history if h.get("source") == "app"]

    # ä»Šæ—¥ã®æŠ•ç¨¿æ•°
    today_count = 0
    for h in app_posts:
        ts = h.get("timestamp", "")
        if ts.startswith(today):
            today_count += 1

    # æœ€è¿‘ã®æŠ•ç¨¿ï¼ˆæ–°ã—ã„é †ã€æœ€å¤§10ä»¶ï¼‰
    recent = app_posts[-10:][::-1]
    recent_posts = []
    for h in recent:
        recent_posts.append({
            "text": h.get("text", "")[:80],
            "timestamp": h.get("timestamp", ""),
            "platform": h.get("platform", ""),
            "style": h.get("style", ""),
        })

    return {
        "today_count": today_count,
        "recent_posts": recent_posts,
    }

